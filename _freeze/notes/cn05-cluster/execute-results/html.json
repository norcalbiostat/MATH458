{
  "hash": "72c59a7ce5687c15375641bc4e14026e",
  "result": {
    "markdown": "---\ntitle: \"Cluster Sampling\"\ndescription: \"When units are not necessarily nicely defined, even when the population is. \"\nauthor: DRAFT\ndate: 4/01/25\nexecute: \n  error: true\n  warning: false\n  message: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse);library(knitr)\nlibrary(sampling); library(survey)\n```\n:::\n\n\n# Introduction\n\n:::{.callout-important}\n#### Sampling Units\nA **psu** or **psus** (plural) is the shortening of the term, primary sampling unit(s). These are the sampling units in a cluster sample. An alternate term is **cluster**.\n\nA **ssu** or **ssus** (plural) is the shortening of the term, secondary sampling unit(s). These are the observation units observed within our clusters.\n:::\n\n### Example of Clustering:\n\nWhen taking a survey of a community, rather than do an SRS of households, you might break up the community into blocks and take an SRS of blocks and sample all or some of the households that fall within those blocks. The blocks are your psus, and the households are your ssus. This method might be less costly if you were doing an in person survey, but might also be less accurate.\n\n## Why use Cluster Sampling?\nClustering is most common for its application.The purpose of stratifying a sample before was to better your prediction or accuracy when the variance varied quite differently among the population. It is not quite the same for Cluster Sampling, for it actually decreases in precision due to some positive correlation between unmeasurable factors that effect certain specific clusters. \n\nWe use Cluster Sampling when: \n1. It may be unfeasible or expensive to construct a sampling frame of the population, and breaking into groups may make sampling doable.\n2. In some populations, it’s less costly to sample in clusters, in terms of travel and time.\n\n![Textbook Figure 5.1. Contrasting stratified sampling with one-stage cluster sampling.](../figs/fig5_1.jpg)\n\n\n## When clusters are Ignored\n\nWhen clusters are ignored in the design or analysis phase, the sample can mistakenly be treated as if it were using the simple random sampling design. When they are ignored they can lead to incorrect formulas that have inaccurate conclusions. For example, when we analyze our sample as an SRS the variability can be underestimated on account of the clusters being designed to be more similar and homogeneous compared to an SRS and stratified sample. This will lead to values that are too small and confidence intervals that can be too narrow.\n\n\n:::{.callout-tip}\n#### Example: Studies in education\nMany studies in education (either involving teachers, students, other faculty, etc.) use cluster sampling for data collection. Clusters can include entire schools or even just individual classrooms. This is much more cost effective and logistically simpler than taking an SRS of individuals (students, staff, etc.).\n\nOne known example [conducted by Basow and Silberg (1987) and Basow and Martin (2012)] examined whether students rated professors differently based on gender (female vs male professors). \n\n:::\n\n* In this study they took 16 female and 16 male professors and matched them by their subjects, experience, and tenure status. \n* They then sampled each of their classrooms full of students for data collection.\n* They, in total, collected 1029 individual student responses but since we are using cluster sampling the sample size will be n = 32 as there are 32 total clusters. The analysis would be wrong if we ignored that it was a cluster sample and treated it as an SRS of size 1029.\n\nThis is a great real world example of how cluster sampling can streamline collecting important data while still gaining valuable insight.\n\n\n# Notation & Formulas\n\nIn SRS, each individual element is the sampling unit, and has an equal chance to be sampled.  In stratified sampling, the population is divided into  $H$ known strata, and a sample is taken from each. Our sampling unit in the cluster design is the entire cluster, or primary sampling unit $psus$, and not the individual elements.\n\nIn stratified sampling we have our entire population being $N$ total population of each individual element, whereas our entire universe in cluster sampling is the number of clusters, that is $N$ is the number of $psus$.\n\n⚠️ Note notation change!\n\n* $y_{ij}$: measurement for the $j$th element in $i$th psu\n* $N$: the number of clusters (psus) in the population\n    * $n$: the number of psus from the sample\n* $M_{i}$: number of ssus in psu $i$ in the population\n    * $m_i$: the number of ssu's in psu $i$ from the sample\n* $M_{0}$: total number of ssus in the population  \n\n| Population Quantity | Estimator $(\\hat{\\theta})$ | Estimated variance of $(\\hat{\\theta})$ |\n|---------------------|----------------------|-----------------------------|\n| Total in psu $i$: $t_{i}=\\sum_{j} y_{ij}$ | $\\hat{t}_{i} = \\frac{M_{i}}{m_i}\\sum_{j} y_{ij}$ | - |\n| Variance of psu totals : $S_t^2 = \\frac{1}{N-1} \\sum_{i} \\left( t_i - \\frac{t}{N} \\right)^2$ | $s_t^2 = \\frac{1}{n-1} \\sum_{i} \\left( t_i - \\frac{\\hat{t}}{N} \\right)^2$ | _AKA variance between psu_|\n| Overall Total: $t = \\sum_{i} t_i$ | $\\hat{t} = \\frac{N}{n}\\sum_{i} \\hat{t}_i$ | $N^2 \\left(1 - \\frac{n}{N}\\right) \\frac{s_t^2}{n}$  |\n| mean in psu $i$: $\\mu_{i} = \\frac{1}{M_i} \\sum_{j} y_{ij}$  | $\\bar{y}_{i} = \\frac{1}{m_i} \\sum_{j} y_{ij}$  | $s_{i}^{2} = \\frac{1}{m_i-1} \\sum_{j} (y_{ij}-\\bar{y}_{i})^2$ (_AKA variance within psu_)|\n| Overall mean: $\\mu = \\frac{1}{M_0} \\sum_{i} \\sum_{j} y_{ij}$  | $\\bar{y} = \\frac{\\hat{t}}{NM}$  | $\\left( 1 - \\frac{n}{N} \\right) \\frac{s_t^2}{nM^2}$ |\n\n\nOverall there are many differences between clustering, stratification, and SRS, that are important to note.\n\n\n\n\n------------------------------------------------------------------------\n\n# One-Stage Clustering Sampling\n\n## Equal size clusters: Estimation\n\nOne-stage clustering is the type of clustering you're probably already familiar with - either all (or none) of the observations inside of a cluster (psu) are sampled.\n\nWhile this is an unrealistic scenario for most statistical work in the social sciences, agricultural or industrial surveys may be able to take advantage of the much simpler process that equal size clustering brings. \n\nBecause the psus are selected through an SRS, we can use the per-cluster estimates as single observations in an SRS. There's no need to consider the individual ssus within each cluster. **Important distinction: we treat this as an SRS of $n$ psus, not an SRS of $nM$ observational units.** \n\nIn this setup, we typically aim to estimate the population total or mean of a variable of interest based on the totals from each sampled cluster.\n\n:::{.callout-tip icon=true}\n### Example 5.2: Average GPA in a dorm\n\nPerhaps we want to estimate the average GPA in a college dormitory with a one-stage cluster design. The dorm consists of 100 units, each with 4 students. Here's the process: \n\n1. An SRS is conducted to select 5 clusters.  \n2. All 4 students (ssus) per dormitory unit (psus) have their GPAs measured\n3. Calculate your estimates based on your *psu* statistics, not your ssu measurements.\n\n_(Slightly modified version of Example 5.2 from Lohr's R Companion to Sampling 3rd ed. Page 57. DOI: 10.1201/9781003228196-5)_\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa <- readr::read_csv(here::here(\"data\", \"gpa.csv\"))\nN <- 100 # total dorm population (this won't be NROW(gpa)!)\nn <- 5   # total number of clusters/psus selected\nM <- 4   # observational units/ssus selected per psu\n```\n:::\n\n\n:::\n\n#### Using equations\n\nTo estimate the mean, we first estimate the total (because it's easier math). \n\nThe estimated total is calculated by a weighted sum of the cluster totals. \n$$\n\\hat{t} = \\frac{N}{n} \\sum_{i \\in S} t_i\n$$\n\nWe can use the `tapply` function to calculate the sum of `gpa` across each `suite`.\n\n::: {.cell}\n\n```{.r .cell-code}\n(suitesum <- tapply(gpa$gpa, gpa$suite, sum))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    1     2     3     4     5 \n12.16 11.36  8.96 12.96 11.08 \n```\n:::\n\n```{.r .cell-code}\n# gpa %>% group_by(suite) %>% summarize(t_i = sum(gpa)) # alternative method\n```\n:::\n\n\nThe variance across these totals can be found using the standard variance formula:\n$$\ns_t^2 = \\frac{1}{n-1} \\sum_{i \\in S} \\left( t_i - \\frac{\\hat{t}}{N} \\right)^2\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(st2 <- round(var(suitesum), digits = 3)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.256\n```\n:::\n:::\n\n\nNow we can convert these estimates of the total to estimates of the mean using:\n\n$$\n\\hat{y} = \\frac{\\hat{t}}{NM} \\qquad \\mbox{ and } \\qquad \\text{SE}(\\hat{y}) = \\frac{1}{M} \\sqrt{\\left(1 - \\frac{n}{N}\\right) \\frac{s_t^2}{n}}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.hat <- sum(suitesum)*(N/n)\n(y.bar.hat <- t.hat/(M*N))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.826\n```\n:::\n\n```{.r .cell-code}\n(se.ybar.yat <- (1/M)*sqrt((1-n/N)*(st2/n)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1636765\n```\n:::\n:::\n\n\n\n#### Using the `survey` package \n\nCreate a survey design for one-stage clustering\n\n::: {.cell}\n\n```{.r .cell-code}\nOSC_gpa <- svydesign(id = ~suite,\n                     weights = ~wt,#already on the data set\n                     fpc = ~rep(100,20),\n                     data = gpa)\n```\n:::\n\n\ncalculate the mean GPA with SE \n\n::: {.cell}\n\n```{.r .cell-code}\nsvymean(~gpa, OSC_gpa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     mean     SE\ngpa 2.826 0.1637\n```\n:::\n:::\n\n\n\n### Sampling Weights\n\nIn one-stage cluster sampling with equal-sized clusters, each cluster is treated as a single observation. The sampling weight for each observation unit $j$ in PSU $i$ is based on the inverse probability of that PSU being selected. This results in a **self-weighting sample**, meaning every sampled element gets the same weight.\n\n$$\nw_{ij} = \\frac{N}{n}\n$$\n\nThe total sampling weight across all sampled units equals the total number of units in the population ($NM$). Make sure to always verify that the sum of your weights matches the population size. Population total and mean can also be estimated using these weights by summing the weighted values or averaging them appropriately. \n\nThis weighting approach allows you to use one-stage cluster sampling estimates similarly to how you'd handle stratified or SRS data, despite the cluster structure.\n\n----\n\n## Equal size clusters: Theory\n\n### Comparing a cluster sample with an SRS of the same size\n\n\n\n:::{.callout-important}\n### ANalysis Of VAriance\nDon't recall how ANOVA works to break up the variance into components? See [this great video](https://www.youtube.com/watch?v=W36DMVJ4Ibo&list=PLkIselvEzpM5G3IO1tzQ-DUThsJKQzQCD) from the Open Intro Statistics author, Mine Cetinkaya-Rundel\n:::\n\n:::{.callout-important}\n### MSB (Mean Squared Between psus)\n\nThe sum of squares between psus (SSB) measures the variability across the psu means. The mean squared between psus (MSB) is the SSB divided by the number of psus -1. \n\n$$\nSSB =  \\sum_{i} \\left( \\bar{y}_i - \\bar{y} \\right)^2  \\qquad  MSB = \\frac{SSB}{N - 1} \n$$\n\nIf MSB is relatively large, it means that there is more of a difference between data points in different clusters than within the same clusters\n:::\n\nFor cluster sampling, the variability of the unbiased estimator of $t$ depends entirely on between the psus rather than within psus.\n\n$$V(\\hat t_{cluster}) = N^2 (1 - \\frac{n}{N}) \\Big( \\frac{M(MSB)}{n}\\Big)$$\n\n\n:::{.callout-important}\n### MSW (Mean Squared Within psus)\n\nThe sum of squares within psus (SSB) measures the variability within the psus. The MSW is the SSW divided by the total number of ssus minus the number of psus.\n\n$$\nSSW = \\sum_{i} \\sum_{j} \\left( y_{ij} - \\bar{y}_i \\right)^2 \\qquad \nMSW = \\frac{SSW}{N(M-1)}\n$$\n\n:::\n\nThe MSW is not used for calculating the variance in cluster sampling like it is in stratified sampling. While stratified sampling is best when there is a lot of variance within strata, cluster sampling is less effective if there's a lot of variance within psus.\n\nIf the ratio of MSB/MSW is large, it means that stratified sampling will be more precise and cluster sampling will be less precise.\n\n\n:::{.callout-important}\n#### ICC (Intraclass Correlation Coefficient)\nThe ICC measures how similar items are within the same group or cluster. It helps determine the usefulness of cluster sampling for a specific sample.\n:::\n\nA high ICC means that items in the same cluster are very similar, indicating a high level of homogeneity. A low ICC means that items within the cluster are more different from each other.\n\nThe higher the ICC, the less efficient the sample is compared to simple random sampling (SRS). This is because high ICC indicates redundancy within clusters. The loss in precision due to clustering can be estimated by comparing the variance of cluster sampling over the variance of the SRS.\n\nICC is typically positive in naturally occurring clusters. For example, wealthy families living in the same neighborhood, or crops in a field exposed to the same pesticide levels, are likely to show positive ICC due to environmental or social factors.\n\nIn contrast, ICC can be negative in artificially constructed or systematic clusters. A negative ICC indicates that items within a cluster are more dispersed than in a randomly selected group. This often results in cluster means being roughly equal, and in such cases, cluster sampling can actually be more efficient than SRS.\n\n\n:::{.callout-important}\n#### Adjusted R Squared\n\nAdjusted R^2 can be used as an alternative measure of homogeneity due to it being very close to the ICC in many populations.  The adj R^2 can only be used as a replacement in cases where all primary sampling units (psus) are the same size. \n\n$$ 1 - \\frac{MSW}{S^2} $$\n\nwhere $S^2$ is the variance calculated as the Total sum of squares (SST = SSB + SSW) divided by the total degrees of freedom. \n:::\n\nYou can extract the values for the degrees of freedom (`Df`), sum of squares (`Sum Sq`), and mean squares (`Mean Sq`) out of the ANOVA table using the following codes. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa$suite <- as.factor(gpa$suite)\n(anova.table <- aov(gpa ~ suite, data = gpa) |> summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)  \nsuite        4  2.256  0.5639   3.048 0.0504 .\nResiduals   15  2.776  0.1850                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nGet total degrees of freedom\n\n::: {.cell}\n\n```{.r .cell-code}\n(df.t <- sum(anova.table[[1]]['Df']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 19\n```\n:::\n:::\n\n\nGet SST\n\n::: {.cell}\n\n```{.r .cell-code}\n(SST <- sum(anova.table[[1]]['Sum Sq']) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.03128\n```\n:::\n:::\n\n\nCalculate $S^2$\n\n::: {.cell}\n\n```{.r .cell-code}\n(s2 <- SST/df.t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2648042\n```\n:::\n:::\n\n\nGet MSW by pulling out the 2nd element in the `Mean Sq` column\n\n::: {.cell}\n\n```{.r .cell-code}\n(MSW <- anova.table[[1]][2, 'Mean Sq'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.18504\n```\n:::\n:::\n\n\nNow calculate adjusted $R^2$\n\n::: {.cell}\n\n```{.r .cell-code}\n1- (MSW / s2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3012196\n```\n:::\n:::\n\n\nSo 30% of the variability in GPA is due to the suite the student lives in. \n\n\n:::{.callout-tip}\n#### Example 5.3. Consider two artificial populations\nRefer to the textbook for numeric specifics\n\n* A & B both come from the same population, and when sampled each has 3 clusters and 3 elements (ssus) inside each cluster. \n* Even though they share the same $S^2$ and $\\bar{y}$ there variances differ in a real specific way. \n    * Population A has a large variance within each cluster which is reflected in the negative ICC and $R^2$. \n    * Whereas, in Population B the variance occurs mostly between clusters. \n:::\n\n\n:::{.callout-tip}\n#### Example 5.5. When is a cluster not a cluster? \n**When it's the whole population!**\n\nIn Example 4.5 (in Chapter 4), we look at data from a study evaluating the effects of feral pig activity and drought on the native vegetation in Santa Cruz Island in California. \n\n:::\n\nIn that example, the _sampling unit_ was a single tree, the _observational unit_ was a single seedling by the tree, and the _population of interest_ was all the trees on Santa Cruz Island. Then, an SRS was conducted! \n\nBut we're doing cluster sampling right now. So, suppose that the researcher was instead interested in seedling survival across all of California. The researcher would have divided the regions into equal-sized areas, randomly-selected some of those areas to study, and recorded the measurements. \n\n**The sampling unit is no longer a tree, it's an area!**\n\nSo, if Santa Cruz Island was selected as one of those _areas_ in our study about all of California, then we could no longer treat the trees from Santa Cruz Island as though they were part of an SRS from all trees in California. They're all similar trees, from the same island, that experience identical climates, in almost the same soil. We would expect all ten trees on Santa Cruz Island to experience, as a group, different environmental factors (such as weather conditions and numbers of seedling eaters) than the ten trees selected in the Santa Ynez Valley on the mainland. Thus the ICC within each cluster (area) would likely be positive.\n\n**But...**\n\nWhat if we were only interested in the seedlings _from_ the 10th tree on Santa Cruz Island? Then, the population is _all seedlings from the 10th tree_ and the psu is the _seedling_. At this point, we're not doing cluster sampling, we're doing a census of the population!\n\n## Unequal size clusters\n\n:::{.callout-tip}\n#### Motivating Example: Census of 1937\n\nIn the Enumerative Check Census of 1937, they took a 2% sample of postal routes (psus), and questionnaires were give to every household(ssus) on each chosen postal routes. Since not all postal routes had the same number of households, this survey was a clustered sample with clusters of unequal sizes. The variable they were investigating, the total number of unemployed people, was impacted within each cluster by the size of the cluster. The more people on a route, the more unemployed people on the postal route. This means that the between variance of the cluster totals would be higher. Something else to note about this survey is that they only knew the number of houses in each of the *selected* groups.\n\n:::\n\nThe **unbiased estimator(s)** for the total are:\n$$\\hat t_{unb} = \\frac{N}{n}\\sum_{i \\in S}t_i \\qquad \\mbox{ with } \\qquad SE(\\hat t_{unb}) = N \\sqrt{(1-\\frac{n}{N})(\\frac{s_t^2}{n})}$$\n\n* Note that the error of the estimator is dependent on the variance of the cluster totals ($s_t^2$) \n* The variation among the individual cluster totals $t_i$ is likely to be large when clusters have different sizes.\n\n$$M_0 = \\sum_{i}^N M_i$$\n\n$$\\hat{\\bar{y}}_{unb} = \\hat t_{unb}/M_0$$\n\n$$SE(\\hat{\\bar{y}}_{unb}) = SE(\\hat t_{unb})/M_0$$\n\n* The size of every psu is needed to be known for this formula.\n* The unbiased estimator for the mean can be inefficient when values of  M_i's are unequal for the same reasons the unbiased estimate of the total can be inefficient\n* A ratio estimator of the mean often has smaller variance.\n\n\n\n### Ratio Estimation\nWe usually expect $t_i$ to be positively correlated with $M_i$. \n\n* Example: If the primary sampling units (PSUs) are counties, we would expect the total number of households living in poverty in county $i(t_i)$ to be roughly proportional to the total number of households in the country $i(M_i)$\n\nTherefore our population mean is a ratio where $t_i$ and $m_i$ are positively correlated. \nWe define that now as: \n\n$$\n\\widehat{\\bar{y_r}} = \\frac{\\hat{t}_{umb}}{\\widehat{M_0}} = \\frac{\\frac{N}{n} \\sum_{i \\in S} t_i}{\\frac{N}{n} \\sum_{i \\in S} M_i} = \\frac{\\sum_{i \\in S} M_i \\bar{y_i} }{\\sum_{i \\in S} M_i}\n$$\n\n* The estimator $\\widehat{\\bar{y_r}}$ is the quantity $B$ in 4.1. (Our sample's ratio mean.)\n\n\nThe denominator is a random quantity that depends on which particular clusters are included in the sample. If the $M_i$ are unequal and a different cluster sample of size $n$ is taken, the denominator will likely be different. From taking $e_i = t_i - M_i \\widehat{ \\bar{y_r}} = M_i (\\bar{y}_i - \\widehat{\\bar{y_i}})$ we have:\n\n$$\\text{SE}(\\widehat{\\bar{y_r}}) = \\sqrt{\\left(1 - \\frac{n}{N}\\right) \\frac{s_r^2}{nM^2}}$$\n\nand, \n\n$$\ns_r^2 = \\frac{1}{n-1} \\sum_{i \\in S} M_i^2 \\left( \\bar{y}_i - \\widehat{\\bar{y_i}}\\right)^2\n$$\n\n\n* $\\bar{M}  = \\frac{\\widehat{M_0}}{N}$ represents the average size of the PSUs in the sample.\n* The variance of the ratio estimator depends on the variability of the means per element in the PSUs and can be much smaller than that of the unbiased estimator $\\hat{\\bar{y}}_{unb}$. \n* This can also be done when we know our cluster sizes.\n\n### Estimation using weights\n\n$$\nw_{ij} = \\frac{1}{\\mbox{probability of w_{ij} being in the sample}} = \\frac{N}{n}\n$$\n\n* One stage clustering has self-weighting samples no matter whether the clusters are equal sizes or not \n* In clusters of equal sizes, the weights sum to the population size but in clusters of unequal sizes the sum of the weights *estimates* the population size.\n\n$$\\hat M_0=\\sum_{i\\in S}\\sum_{j \\in S_i}w_{ij} = \\frac{N}{n}\\sum_{i \\in S}$$\n\n* $\\hat M_0$ is a random variable, different for every sample.\n\nUsing weights:\n\n$$\\hat t_{unb} = \\sum_{i\\in S}\\sum_{j \\in S_i}w_{ij}y_{ij}$$\n\n$$\\hat{\\bar{y_r}}= \\frac{\\sum_{i\\in S}\\sum_{j \\in S_i}w_{ij}y_{ij}}{\\sum_{i\\in S}\\sum_{j \\in S_i}w_{ij}}$$\n\n:::{.callout-tip icon=false}\n### Example. High school Algebra\n\nConsider a population of 187 high school algebra classes in a city. An investigator takes an SRS of classes, and gives each student in the sampled classes a test about function knowledge. The (hypothetical) data are given in the file `algebra.csv`. \n:::\n\na. Load in the data and look at the first few rows. Explain what the values in each column mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgebra <- readr::read_csv(here::here(\"data\", \"algebra.csv\"))\nhead(algebra)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  class    Mi score\n  <dbl> <dbl> <dbl>\n1    23    20    57\n2    23    20    90\n3    23    20    56\n4    23    20    57\n5    23    20    46\n6    23    20    55\n```\n:::\n:::\n\n\n* Each row is a student, with three columns: `class`, `Mi`, and `score`.\n* `class` is a number representing which class the student is in. These are the psus.\n* `Mi` is the number of students in that specific class. For class 23, there are 20 students.\n* `score` is the score the student earned on the given test.\n\n\nb. Plot the distribution of test score by class. Interpret your graph by comparing the medians, ranges, and variances across classes. What do you notice?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgebra$class <- as.factor(algebra$class)\nggplot(algebra, aes(x = class, y = score)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cn05-cluster_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nalgebra %>% group_by(class) %>% summarise(median = median(score),\n                                          min = min(score),\n                                          max = max(score),\n                                          var = var(score))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 × 5\n   class median   min   max   var\n   <fct>  <dbl> <dbl> <dbl> <dbl>\n 1 23      58      25    90  310.\n 2 37      64      24   100  461.\n 3 38      58.5    24   100  420.\n 4 39      54      32    89  200.\n 5 41      58.5    19    87  188.\n 6 44      64.5    27   100  193.\n 7 46      49      34    88  253.\n 8 51      71.5    22   100  329.\n 9 58      51      28   100  546.\n10 62      71      31    99  282.\n11 106     67.5    14    94  355.\n12 108     69.5    31   100  227.\n```\n:::\n:::\n\n\nThere is a large range between the minimum and the maximum score in each class, with all of the minimums being between 14 and 34 and all of the maximums being between 87 and 100. We can also see some pretty large differences in median between classes, for example Class 46 has a median score of 49.0 and Class 51 has a median score of 71.5. For variance, the lowest class is Class 41 with about 188, and the largest class is Class 58 with about 546.\n\nc. How many _psus_ are there? What is the _psu_ with the highest number of _ssus_? Which one has the least? You must answer this question using R functions such as`unique` or `table`. Don't count this by hand (practice for when you have a billion rows)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(n <- length(unique(algebra$class))) # Total number of psu\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 12\n```\n:::\n\n```{.r .cell-code}\npsu <- table(algebra$class)\n\n(psu.max.ssu<- names(which.max(psu))) # PSU with most SSUs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"39\"\n```\n:::\n\n```{.r .cell-code}\n(max.ssu<- max(algebra$Mi)) #This shows the max SSU\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 34\n```\n:::\n\n```{.r .cell-code}\n(psu.min.ssu<- names(which.min(psu))) # PSU with fewest SSUs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"58\"\n```\n:::\n\n```{.r .cell-code}\n(min.ssu<-min(algebra$Mi)) #This shows the minimum SSU\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17\n```\n:::\n:::\n\n\nThere are 12 psus which are the classes. The psu with the highest number of ssus is class 39 with 34 ssus, and the smallest ssus is class 58 with 17 ssus.\n\nd. Add variables containing the weights and the `fpc` to this data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 187\nalgebra <- algebra %>%\n  mutate(wt = N/n, \n         fpc=rep(187,299))\n```\n:::\n\n\ne. Using the proper functions from the `survey` package, estimate the population average algebra score, with a 95% interval and interpret your interval. Be sure to NOT assume a Z-distribution for this confidence interval but use the `degf` function to get the proper degrees of freedom out of the survey design object and pass it to the `confint` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndesign <- svydesign(\n  id = ~class,\n  fpc = ~Mi,\n  weights = ~wt, \n  data = algebra\n)\n\n# the mean\nmean_estimate <- svymean(~score, design)\n\n# degrees of freedom function\ndf <- degf(design)\n\n#conf. interval, t distribution\nci <- confint(mean_estimate, df = df)\n\n(mean_estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        mean     SE\nscore 62.569 0.9752\n```\n:::\n\n```{.r .cell-code}\n(ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         2.5 %   97.5 %\nscore 60.42224 64.71488\n```\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n# Two-Stage Clustering Sampling\n\n![Textbook Figure 5.2. Differences between one-stage and two-stage cluster sampling.](../figs/fig5_2.jpg)\n\n## Unbiased estimator of population total.\n\nTo estimate the total for each PSU ($\\hat{t_i}$) we multiply the total number of SSUs in a PSU ($i$) *by* the average of the sampled SSUs in a PSU ($i$). This is then used in equation (5.21) to get the unbiased estimate of the total population. This is essentially scaling up the estimates using the known sizes:\n\n* $N$ (total number of PSUs in the population)\n* $n$ (Number of PSUs sampled)\n* $M_i$ (total number of SSUs in PSU_i)\n* $m_i$ (number of SSUs actually sampled in PSU_i). \n\n## Survey weights\nThe chance of selecting any SSU depends on:\n\n1. The chance that its PSU was selected in the first stage.\n2. The chance that the SSU was selected in the second stage within that PSU.\n    \nThe sampling weight for each SSU is the reciprocal of its selection probability. It tells you how many units in the population that one SSU is meant to represent.\n\nIf all PSUs sample SSUs in proportion to their size, the design is self-weighting.\n\n$$w_i = NM_i / nm_i$$ \n\n## Variance estimation for two-stage cluster samples.\n\nVariance $\\hat{t}$ has 2 components: the variability between psu's and the variability of of ssus within psu's.( We don't have to worry about this with one stage clustering.) \n\nEssentially what the equation is doing is proving that the second term in the 5.24 equation goes to zero so you don't need to account for this. The equation below calculates the variance for the sample variance among psu totals and then calcs the sample variance within the psu i.\n\n\n:::{.callout-tip icon=true}\n### Example 5.7: More estimation of math scores. \n\nThe file `schools.csv` contains data from a two-stage sample of students from hypothetical classes. The final weights are provided in the variable `finalwt`. \n\n- During the first stage of sampling, an SRS of size n = 10 schools was selected from a population of N = 75 schools overall. \n- During the second stage of sampling, an SRS of size $m_i$ = 20 students was selected from each sampled school.\n- Tests for reading and math scores were administered for individuals. \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschools <- readr::read_csv(here::here(\"data\", \"schools.csv\"))\n#factorizing our school ids\nschools$schoolid <- as.factor(schools$schoolid)\n```\n:::\n\n\n**a) Create side by side boxplots for the math score for sampled schools. Which do you think is larger, the within or between cluster variance?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(schools, aes(x = reorder(schoolid, math, FUN = median), \n                    y = math, fill = schoolid)) +\n  geom_boxplot() +\n  labs(x = \"School ID\", y = \"Math Score\") \n```\n\n::: {.cell-output-display}\n![](cn05-cluster_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\nFrom the way the medians of each of the schools differ from each other greatly, it seems that the between cluster variance is greater than the within cluster variance.\n\n**b) Create an ANOVA table for this example. Is your interpretation from the graph upheld by this analysis?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(math ~ schoolid, data=schools) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nschoolid      9   7018   779.8   7.583 1.79e-09 ***\nResiduals   190  19538   102.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nYes. The Mean Sq for the first row of the Anova table is the between clusters mean squares(MSB), while the second row's Mean Sq is the within clusters mean squared (MSW). Since the MSB is seven times the MSW, the Anova table supports our hypothesis that the variance between clusters(schools in this example) is greater than the variance within clusters.\n\n\n**c) Estimate with proper 95% CI the average math score for the population of students under consideration.**\n\nSince we don't have the knowledge about the population to calculate the fpc, we will treat this sample as if it were taken with replacement and not use the finite population corrector in the survey design for using the survey package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndschools <- svydesign(id=~schoolid,\n                      weights=~finalwt,\n                      data=schools)\n```\n:::\n\n\nWe will save the `mathmean` information to use to calculate our confidence interval\n\n::: {.cell}\n\n```{.r .cell-code}\nmathmean <- svymean(~math,dschools)\nmathmean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       mean     SE\nmath 33.123 1.7599\n```\n:::\n:::\n\n\nThis is the confidence interval using the t-distribution with 9 degrees of freedom since we have 10 schools, or clusters, within our sample. Note that our degrees of freedom is the number of psus - 1, not the number of ssus - 1. This is a cluster sample, not an SRS.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(mathmean,df=degf(dschools))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        2.5 %  97.5 %\nmath 29.14179 37.1041\n```\n:::\n:::\n\n\nOur sample mean math score is 33. We are 95% confident that the true average math score for the population we are considering is between 29 and 37.\n\n\n\n\n\n\n",
    "supporting": [
      "cn05-cluster_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}