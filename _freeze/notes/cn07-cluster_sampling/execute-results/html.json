{
  "hash": "8014f238ff381a3bf1f39877ec8bd2c5",
  "result": {
    "markdown": "---\ntitle: \"Cluster Sampling\"\ndescription: \"Selecting groups at . Lohr Ch 5. \"\nauthor: 458 class\ndate: 3/20/23\nexecute: \n  error: true\n  warning: false\n  message: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here); library(tidyverse);library(knitr)\nlibrary(sampling); library(survey)\n```\n:::\n\n\n# Introduction\n\nLet’s say we want to find the number of bicycles owned within a community of 10,000 households. We have several options for picking a sample:\n\n- Obtaining a SRS of 400 households\n- Stratifying households into groups then choosing a SRS from these groups \n- Separate the population into blocks of 500 and then randomly select 20 of these blocks and survey every household that is part of the blocks chosen\n\n This last option is an example of **cluster sampling**. In cluster sampling, we have **primary sampling units**(psus), or clusters, which are represented by the blocks. We also have **secondary sampling units**(ssus) which are the households within the blocks. In an SRS, the units sampled are the observed elements, but in a cluster sample, the sampling units are our clusters. In a cluster sample, our universe is the population $N$ clusters. \n\nCluster of 400 households vs SRS of 400 households?\n\n- The cluster sample would result in less precision because of our blocks. Some might be composed of mainly families(who are more likely to have bikes) while others are mainly older people.\n- In this case, clusters won’t be as representative of the population as a SRS would. \n- Can be cheaper and faster than other sampling methods\n\n\n![Textbook Figure 5.1. Contrasting stratified sampling with one-stage cluster sampling.](../figs/fig5_1.jpg)\n\n\n# Notation & Formulas\n\n::: panel-tabset\n## Pop psu level\n\n| Symbol      | Formula                                                          | Description                                    |\n|------------|--------------------------|----------------------------------------|\n| $y_{ij}$    |                                                                  | measurement for the $j$th element in $i$th psu |\n| $N$         |                                                                  | number of clusters (psus) in the population    |\n| $M_{i}$     |                                                                  | number of ssus in psu $i$                      |\n| $M_{0}$     | $\\sum_{i=1}^{N} M_{i}$                                           | total number of ssus in the population         |\n| $t_{i}$     | $\\sum_{j=1}^{M_{i}} y_{ij}$                                      | total in psu $i$                               |\n| $t$         | $\\sum_{i=1}^{N} t_{i}= \\sum_{i=1}^{N} \\sum_{j=1}^{M_{i}} y_{ij}$ | population total                               |\n| $S_{t}^{2}$ | $\\frac{1}{N-1} \\sum_{i=1}^{N} (t_{i} - \\frac{t}{N})^2$           | population variance of the psu totals          |\n\n## Pop ssu level\n\n| Symbol           | Formula                                                                  | Description                        |\n|-------------------|-----------------------------------|-------------------|\n| $\\bar{y}_{\\mu}$  | $\\frac{1}{M_0} \\sum_{i=1}^N \\sum_{j=1}^{M_i} y_{ij}$                     | population mean                    |\n| $\\bar{y}_{i\\mu}$ | $\\frac{1}{M_i} \\sum_{j=1}^{M_i} y_{ij} = \\frac{t_i}{M_i}$                | population mean in psu $i$         |\n| $S^2$            | $\\frac{1}{M_0-1} \\sum_{i=1}^N \\sum_{j=1}^{M_i} (y_{ij}-\\bar{y}_{\\mu})^2$ | population variance (per ssu)      |\n| $S_{i}^{2}$      | $\\frac{1}{M_i-1} \\sum_{j=1}^{M_i} (y_{ij}-\\bar{y}_{\\mu})^2$              | population variance within psu $i$ |\n\n## Sample\n| Symbol                   | Formula                                                  | Description                                     |\n|------------------|-----------------------------|-----------------------------|\n| $y_{ij}$ |                                                                |Measurement for$j$th element in $i$ht psu                                             |\n|$N$||Number of clusters (psus) in the population|\n|$M_i$||Number of ssus in psu $i$|\n|$M_{o}$|$\\sum_{i=1}^NM_i$|Total number of psus in the popultion|\n|$t_i$|$\\sum_{j=1}^{M_i}y_{ij}$|Total in psu $i$|\n|$t$|$\\sum_{i=1}^Nt_i = \\sum_{i=1}^N\\sum_{j=i}^{M_i}y_{ij}$|Popultion Total|\n|$S_i^2$|$\\frac{1}{N-1} \\sum_{i=1}^N(t_i-\\frac{t}{N})^2$|Popultion variance of the psu totals|\n\n:::\n\n\n\n## Why use Cluster Sampling?\n\n-   Constructing a sample frame list may be expensive, difficult, or impossible.\n    -   Making a sampling frame of all honeybees in a region would be impossible-you cannot enumerate them, or even be sure how many there are (N)\n    -   Making a sampling frame of all trees in a region is possible, however it may be extremely time consuming, and therefore expensive.\n-   The population may be widely distributed geographically or appear in natural clusters.\n    -   Households or schools can appear in natural clusters\n    -   If your target population is nursing home residents in the US, it would be very hard to take an SRS of all nursing home residents, and then travel to each one individually. Taking a cluster sample of a few nursing homes and then taking observations from everyone in them would be less expensive and time consuming.\n\n:::{.callout-note icon=false}\nA cluster is similar to a stratum because it is grouping members of the population.\n\nHowever, cluster sampling reduces precision compared to SRS whereas precision is increased with a stratified sample. \n:::\n\nThis is because members of the same cluster tend to be very similar, and sampling everyone in the cluster results in the same information being repeated rather than learning new information.\n\nMost large surveys usually use cluster sampling, most likely because of the low costs and convenience compared to other sampling methods\n\nTo increase the precision of the clusters, survey conductors could group the population primary sampling units (the clusters) into stratas and select a probability sample of psus within each stratum. \n\n## When clusters are Ignored\n\nDo not analyze cluster samples like you would analyze an SRS. Doing so can lead to statistics from the survey appearing more precise than they actually are.\n\n:::success\n### Studies in education\n\nDue to the natural clustering of students within a classroom, educational studies often involve cluster sampling. \n\nConsider a study where a school (the population) is stratified by classroom. 32 professors (16 male and 16 female) were selected based on the subject they taught, years of experience, and tenure status. In this example, the classrooms are the clusters.\n:::\n\nQuestionnaires went out to all students in each of the selected classrooms. The researchers wanted to see if students evaluated male and female professors differently.\n\n* The sample size in this study is $n = 32$, the number of faculty studied\n    * Note this is **not** the total number of students who returned the questionnaire.\n* Because the students in each classroom are likely to have some agreement in the rating they assign to their professor, they can not be treated as independent.\n    * If the students are treated as independent, the variance from the sample will be _smaller_ than the real value and differences between clusters will be declared statistically significant more often than they should be.\n\n\n------------------------------------------------------------------------\n\n# One-Stage Clustering Sampling\n\n## Equal size clusters\n\n-   Each *psu* has the same number of elements; $M_{i} = m_{i} = M$.\n-   Often can be found in agriculture or industrial sampling.\n-   $t_{i}$ is the total for all elements in *psu* $i$.\n-   $t_{i}$ is known for sampled households, since data is collected on both individuals. (E.g. $Var(t_{i})=0$)\n\n:arrow_right: Treat the *psu* means or totals as observations, and ignore the individual elements. We now have an SRS of $n$ data points.\n\nThus we can use the same SRS formulas to estimate the total and mean, **however** this is an SRS of $n$ *psu*'s, not an SRS of $nM$ observational units. So the $df$ needed to create a Confidence Interval is $n-1$.\n\nTo estimate the population total $t$ or average $\\mu$, we can use the following estimators and variances:\n\n$$\n\\hat{t} = \\frac{N}{n}\\sum_{i \\in S}t_{i} \\qquad \ns_{t}^{2} = \\frac{1}{n-1}\\sum_{i \\in S}\\Big(t_{i} - \\frac{\\hat{t}}{N}\\Big)^2\n$$\n\nand \n$$\n\\hat{\\bar{y}} = \\frac{\\hat{t}}{NM} \\qquad\nV(\\hat{\\bar{y}}) = \\Big(1-\\frac{n}{N}\\Big)\\frac{s^{2}_t}{nM^2}\n$$\n\n::: {.callout-tip icon=\"true\"}\n### Estimate the income in a two person households\n\nConsider individual incomes ($y_{ij}$) for each of $j=1,2$ persons in the 3 sampled households out of the 50 in the population.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhhi <- data.frame(\n  household = c(1:3), \n  yi1 = c(37638, 69012, 30001), \n  yi2 = c(79999, 86753, 54321)\n)\nkable(hhi, align = 'c')\n```\n\n::: {.cell-output-display}\n| household |  yi1  |  yi2  |\n|:---------:|:-----:|:-----:|\n|     1     | 37638 | 79999 |\n|     2     | 69012 | 86753 |\n|     3     | 30001 | 54321 |\n:::\n:::\n\n:::\n\nFirst define our known values: N = 50, n = 3, M = 2.\n\nThe total household income $t_i$ is calculated as the sum of the individual incomes within each cluster $\\sum_j y_{ij}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhhi$ti <- hhi$yi1 + hhi$yi2\nhhi\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  household   yi1   yi2     ti\n1         1 37638 79999 117637\n2         2 69012 86753 155765\n3         3 30001 54321  84322\n```\n:::\n:::\n\n\nThe estimated mean is\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.hat <- (50/3)*sum(hhi$ti)\n(ybar.hat <- t.hat / (50*2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 59620.67\n```\n:::\n:::\n\n\nwith variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns2.t <- (1/(3-1))*sum((hhi$ti - t.hat/50)^2)\n(var.ybar.hat <- (1 - 3/50)*(s2.t/(3*2^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100106551\n```\n:::\n:::\n\n\nCalculate CI.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrit.t <- qt(.975, 3-1)\nci.low.mu  <- ybar.hat - crit.t*sqrt(var.ybar.hat)\nci.hi.mu   <- ybar.hat + crit.t*sqrt(var.ybar.hat)\n```\n:::\n\n\nWe can be 95% confident that the average population income for a household of two people is contained in the interval ($16,571, $102,670.\n\n:::{.callout-tip icon=true}\n### Example: Average GPA in a dorm\n\nA student wants to estimate the average GPA in their dorm. Obtaining a listing of all students in the hall and conducting an SRS would take a lot of time. Instead, since each of the 100 suites in the hall have 4 students, the student randomly samples 5 suites and collects GPA data for each student in the suite.\n\nUsing functions from the `sampling` package, estimate the average GPA for all students living in that dorm, with a proper 95% CI. Interpret your interval.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa <- readr::read_csv(here::here(\"data\", \"gpa.csv\"))\nhead(gpa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  suite   gpa    wt\n  <dbl> <dbl> <dbl>\n1     1  3.08    20\n2     1  2.6     20\n3     1  3.44    20\n4     1  3.04    20\n5     2  2.36    20\n6     2  3.04    20\n```\n:::\n:::\n\n\nThe data was recorded in long format -- this is the format needed for analysis. The table in the book is formatted for human eyeball consumption, not for analysis.\n:::\n\n1.  Set the survey design. Here we finally use the `id` argument to identify the variable containing the _psu_'s. Printing out the object `gpa.design` lets us confirm that it is being recognized as a 1 stage cluster design with 5 clusters. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n(gpa.design <- svydesign(id = ~suite,\n                        weights = ~wt,\n                        fpc=~rep(100,20),\n                        data=gpa) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 - level Cluster Sampling design\nWith (5) clusters.\nsvydesign(id = ~suite, weights = ~wt, fpc = ~rep(100, 20), data = gpa)\n```\n:::\n:::\n\n\n2.  Call `surveymean` and CI.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(dorm.gpa.mean <- svymean(~gpa, gpa.design))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     mean     SE\ngpa 2.826 0.1637\n```\n:::\n\n```{.r .cell-code}\n(df <- degf(gpa.design)) # Extract degrees of freedom\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n\n```{.r .cell-code}\nconfint(dorm.gpa.mean, level=.95, df=4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2.5 %   97.5 %\ngpa 2.371593 3.280407\n```\n:::\n:::\n\n\nWe can be 95% confident that the average GPA for students living in this dorm is contained in the interval 2.37 to 3.28.\n\n:::{.callout-warning icon=false}\n### :star: You try it. High school Algebra\nConsider a population of 187 high school algebra classes in a city. An investigator takes an SRS of classes, and gives each student in the sampled classes a test about function knowledge. The (hypothetical) data are given in the file `algebra.csv`. \n\na. Load in the data and look at the first few rows. Explain what the values in each column mean.\nb. Plot the distribution of test score by class. Interpret your graph by comparing the medians, ranges, and variances across classes. What do you notice?\nc. How many _psus_ are there? What is the _psu_ with the highest number of _ssus_? Which one has the least? You must answer this question using R functions such as`unique` or `table`. Don't count this by hand (practice for when you have a billion rows)\nd. Add variables containing the weights and the `fpc` to this data set. Assign the value of 187 to a new variable for the fpc. \ne. Using the proper functions from the `survey` package, estimate the population average algebra score, with a 95% interval and interpret your interval. Be sure to NOT assume a Z-distribution for this confidence interval but use the `degf` function to get the proper degrees of freedom out of the survey design object and pass it to the `confint` function. \n:::\n\n:::{.callout-warning icon=false collapse=\"true\" appearance=minimal}\n### Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nyay.math <- read_csv(here::here(\"data/algebra.csv\"))\nhead(yay.math) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  class    Mi score\n  <dbl> <dbl> <dbl>\n1    23    20    57\n2    23    20    90\n3    23    20    56\n4    23    20    57\n5    23    20    46\n6    23    20    55\n```\n:::\n:::\n\n\n**a)**\n\n* `class`: Class number. This is a `psu`. \n* `Mi`: Number of `ssus` in each `psu`. \n* `score`: Test score for each `ssu`\n\n**b)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(yay.math, aes(y=score, x=as.factor(class))) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cn07-cluster_sampling_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe distribution of test scores are similar-ish. Most range between 50 and 75, with class number 51 having the highest median, but also nearly the lowest minimum. The variances of classes 39, 41, and 44 have the smaller variance compared to the other classes. \n\n**c)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(unique(yay.math$class))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 12\n```\n:::\n\n```{.r .cell-code}\ntable(yay.math$class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n 23  37  38  39  41  44  46  51  58  62 106 108 \n 20  26  24  34  26  28  19  32  17  21  26  26 \n```\n:::\n:::\n\n\nThere are 12 different classes, with class sizes ranging from 17 in class number 58 to 34 in class number 39. \n\n**d)**\nSurvey weights were not included, so those need to be added. For a single cluster sample, the weights are calculated as $w_{ij} = \\frac{N}{n}$, the number of clusters in the population divided by the number of clusters in the sample. Here this is 187/12. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nyay.math$wt <- 187/12\nyay.math$fpc <- 187\n\nyay.math.design <- svydesign(id = ~class, weights = ~wt, \n                             fpc = ~fpc,  data = yay.math)\nyay.math.design\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 - level Cluster Sampling design\nWith (12) clusters.\nsvydesign(id = ~class, weights = ~wt, fpc = ~fpc, data = yay.math)\n```\n:::\n:::\n\n\n**e)**\n\n::: {.cell}\n\n```{.r .cell-code}\nsvymean(~score, yay.math.design)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        mean     SE\nscore 62.569 1.4916\n```\n:::\n\n```{.r .cell-code}\nsvymean(~score, yay.math.design) %>% confint(df = degf(yay.math.design))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         2.5 %  97.5 %\nscore 59.28562 65.8515\n```\n:::\n:::\n\n\nThe population average math score is 62.6 (95% CI 59.3 - 65.9). \n\n:::\n\n\n------------------------------------------------------------------------\n\n# Two-Stage Clustering Sampling\n\nSometimes measurements on individual ssus are not easy to obtain, or the ssus within a psu are so similar that collecting data on all of them is a waste of resources. \n\nTwo-stage clustering employes **another** SRS of ssus within each psu. Or, more formally; \n\n1. Select an SRS of $n$ psus from the population of N psus. \n2. Select an SRS of ssus from each selected psu, where $m_{i}$ is the number of ssus selected from the $i^{th}$ psu. \n\nThe point estimates for parameters such as the total or mean will still be calculated the same, but the variance equations now need to take into consideration the variance of the SRS of psus, AND the variance of the SRS of the ssus. Another way to think about it is that we have within-psu and a between-psu components to the variance now. \n\nThis means we can use ANOVA to calculate and understand the ratio of the between and within variance components. \n\n### Survey weights\n\nSince the two SRS's are independent, we can calculate the inclusion probability as the multiplication of the inclusion probability at each stage. \n\n* Stage 1: $P$($i$th psu selected) = $\\frac{n}{N}$. \n* Stage 2: $P$($j$th ssu selected | $i$th psu selected) = $\\frac{m_i}{M_i}$\n\nSince the weights are the reciprocal of the inclusion probability, \n$$\nw_{ij} = \\frac{N}{n}\\frac{M_{i}}{m_{i}}\n$$\n\n:::{.callout-tip icon=true}\n### Example: More estimation of math scores. \nThe file `schools.csv` contains data from a two-stage sample of students from hypothetical classes. The final weights are provided in the variable `finalwt`. \n\na) Create side by side boxplots for the math score for sampled schools. Which do you think is larger, the within or between cluster variance? \nb) Create an ANOVA table for this example. Is your interpretation from the graph upheld by this analysis? \nc) Estimate with proper 95% CI the average math score for the population of students under consideration. _Do not include a value for the `fpc` in this example_. \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschools <- read_csv(here::here(\"data/schools.csv\"))\nhead(schools)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  schoolid gender  math reading mathlevel readlevel    Mi finalwt\n     <dbl> <chr>  <dbl>   <dbl>     <dbl>     <dbl> <dbl>   <dbl>\n1        9 F         42      42         2         2   163    61.1\n2        9 F         29      30         1         1   163    61.1\n3        9 M         31      25         1         1   163    61.1\n4        9 F         22      33         1         2   163    61.1\n5        9 M         35      36         1         2   163    61.1\n6        9 F         30      17         1         1   163    61.1\n```\n:::\n:::\n\n\n**a)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(schools, aes(y=math, x=as.factor(schoolid))) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](cn07-cluster_sampling_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nMath scores within school seem to be more similar than math scores across schools. \n\n**b)**\n\n::: {.cell}\n\n```{.r .cell-code}\naov(math ~ as.factor(schoolid), data = schools) %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     Df Sum Sq Mean Sq F value   Pr(>F)    \nas.factor(schoolid)   9   7018   779.8   7.583 1.79e-09 ***\nResiduals           190  19538   102.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe within cluster variation (MSE) is smaller than the between cluster variation (MSB). This matches my interpretation of the prior graph. \n\n**c)**\n\n::: {.cell}\n\n```{.r .cell-code}\nsch.dsgn <- svydesign(id = ~schoolid, weights = ~finalwt, data = schools)\nsvymean(~math, sch.dsgn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       mean     SE\nmath 33.123 1.7599\n```\n:::\n\n```{.r .cell-code}\nsvymean(~math, sch.dsgn) %>% confint(df = degf(sch.dsgn))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        2.5 %  97.5 %\nmath 29.14179 37.1041\n```\n:::\n:::\n\n\nThe average math score in this example is 33.1 with 95% CI 29.1-37.1\n\n\n## Summary of One vs Two Stage Clustering\n\n![Textbook Figure 5.2. Differences between one-stage and two-stage cluster sampling.](../figs/fig5_2.jpg)\n\n**One Stage Clustering**\n\n* All you do is carry out an SRS on all available PSUs from your population of $N$ PSUs. Then subsequently establish a census.\n* This provides us the variance across clusters\n\n**Two Stage Clustering**\n\n* In two stage clustering the only additional step is to take an SRS of size $m_i$ within each already sample $i$th PSU.\n* This finds the variance within clusters\n\n**Unbiased Estimate for $t$**\n\n$$t=w_{ij}\\sum y_{ij}=\\sum\\frac{NM_i}{nm_i}y_{ij}$$\n\nNote: The same formula is applicable to both one and two stage clustering because, if you think about it, one stage clustering is just a special case of two stage clustering.  That being the case where $M_i=m_i$.\n\n\n\n------------------------------------------------------------------------\n\n# Systematic Sampling\n\nSystematic Sampling is a special case of Cluster Sampling\n\n## Method\n* Determine a desired sample size, X\n* Determine the number of elements in the target population\n* Chose a number of desired psus, Y\n* Now draw an element and every Xth elements afterwards\n    * Do that Y times so that you are left with Y psus \n* Then SRS one of the psus\n\n## Attributes to Note \n* Systematic sample is more precise than SRS when the variance within the PSUs are larger than the variance of the total population.\n* In systematic sampling, we usually only SRS one psu, meaning n=1, so we cannot find the variance of population estimates.\n* So, we need to identify the structure of the population of interest to estimate the variance\n\n:::{.callout-tip icon=true}\n### Example: Choosing a Systematic Sample\n\nSuppose we want to take a sample of 3 from a population of 12 that is numbered 1-12. \n\n* Choose a random number between one and 4 and draw that element and also every fourth element after that. \n    * Do that until all your suss have been grouped\n* Now we have four _psus_ each containing 3 _ssus_ \n* Take an SRS of one _psu_ to use as your sample. \n\n:::\n\n## Order matters\n\nHow does the order of the sampling frame influence estimates?\n\n**The sampling frame is in random order**\n\n*    Systematic sampling is likely to produce a sample that behaves like an SRS. \n*    Oftentimes, the ordering of the population is unrelated to the characteristic of interest.\n*    We can use SRS results and formulas to calculate the variance of a systematic sample.\n\n**The sampling frame is increasing or decreasing in order**\n\n*    Systematic sampling is likely to be more precise than SRS.\n*    Forces the sample values to be more spread out; an SRS could consist of mostly low or high values.\n*    Can use SRS formulas to calculate SE, but it will likely be an overestimate.\n\n**The sampling frame has a periodic pattern**\n\n*    Less precise than SRS\n*    Most dangerous when the population is cyclical or periodic order, and the sampling interval coincides with a multiple of the period\n\nIf periodicity is a concern, using **interpenetrating systematic samples** may be beneficial. \n\n* Take multiple systematic samples from the population, then use formulas for cluster sampling to estimate the desired point estimates.\n\n\n:::{.callout-tip icon=true}\n### Example: sampling toxic waste\nMany dumps and landfills contan hazardous waste that was sealed when it was deposited, however may have began leaking since it was dumped. Since we dont know where the waste was deposited, in order to know if there is any leakage of waste, a systematic sample can be used. \n\n1. Choose a point in a random area, and construct a grid containing that point so that the grid points are equal distance away from each other\n2. take soil samples from each to see if there are traces of hazardous waste\n:::\n\n![Example of sampling Toxic Waste sites](../figs/toxic_waste.png)\n\n\n**Pros and Cons of Systematic Sampling in this example**: \nforces even coverage of the reason, and it is easy to implement in the field. If you have little knowledge about the exact location of what you are looking for. \n\nAs with any grid sample, you need to worry about whether or not the hazardous waste is reglarly placed so that the grid would miss all of it. \n\n\n:::{.callout-important}\n## Definition: Interpenetrating Sampling\nModify a Systematic Sampling approach by \n\n1. change the starting point and keep the interval, \n2. change the interval and keep the starting point, \n3. or change both\n:::\n\n* Used if we are concerned about periodicity \n* Any of these changes force more coverage of the total population vs a systematic sample\n* Instead of taking one systematic sample from our population, we take several\n* In this case, each systematic sample acts as a cluster\n* We can use formulas for cluster samples to estimate variance\n\n    \n------------------------------------------------------------------------\n\n# Clusters of Equal Sizes (Theory)\n\nThis section compares the analysis of variance in cluster sampling compared to stratified and SRS. \n\n* Cluster sampling almost always has less precision for the estimates than from an SRS with the same amount of elements!\n* In one-stage cluster sampling, the variability of an unbiased estimator depends on the between psu variability.\n* We can use an ANOVA table to explore the breakdown of the source of variance into between _psus_\n and within _psus._ \n \n:::{.callout-important}\n### ANalysis Of VAriance\nDon't recall how ANOVA works to break up the variance into components? See [this great video](https://www.youtube.com/watch?v=W36DMVJ4Ibo&list=PLkIselvEzpM5G3IO1tzQ-DUThsJKQzQCD) from the Open Intro Statistics author, Mine Cetinkaya-Rundel\n:::\n \n### MSB (Mean Squared Between Clusters)\n\n* A measure of between cluster variability\n* In stratification, increase in MSB increases precision\n* In clustering, increase in MSB decreases precision\n    * MSB is relatively large because it measures the cluster-to-cluster variability. Also, the MSB can be too large when unmeasured factors affect the overall mean for a cluster\n\n### MSW (Mean Squared Within Clusters)\n\n* A measure of within cluster variability\n* When clusters are relatively homogeneous, the MSW will be small\n\n\n:::{.callout-note appearance=minimal}\nWhen MSB/MSW is large, precision decreases! Better off doing an SRS of the same size.\n:::\n\n## ICC (Intraclass Correlation Coefficient)\n\nLet’s us know how similar elements are in the same cluster; a measure of homogeneity within the cluster\n\n* With a positive ICC, elements within the same cluster are similar. A negative ICC indicates the elements within a cluster to not be similar; elements are dispersed more.\n    * Cluster sampling is less efficient than SRS of elements\n* With a negative ICC, then the elements within a cluster are dispersed more than a randomly chosen group\n* Naturally occurring clusters is the cause for an ICC to be positive, usually not negative\n\n## R Squared\n\nAdjusted $R^2$ is for an alternative measure of homogeneity in general populations.\n\n* $R^2$ can be used to measure homogeneity because its interpretation in linear regression\n* When the _psus_ are homogeneous then the _psu_ means will be highly variable relative to the variation within _psus_, and $R^2$ will be high.\n\n--------------------------------------------------------\n\n# Designing a Cluster Sample\n\nA psu that is too large may eliminate the cost related benefits of clustering\n\n* Step 1: What overall precision is needed?\n* Step 2: What size should the psus be?\n* Step 3: How many ssus should be sampled in each psu selected for the sample?\n* Step 4: How many psus should be sampled?\n\nAny survey design must know the answer to question 1, and to answer questions 2-4 the cost of sampling a psu for potential psus, the cost of sampling a ssu, and a measure of homogeneity (ICC) for potential sizes of psu must be known.\n\n\n\n",
    "supporting": [
      "cn07-cluster_sampling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}