{
  "hash": "6925f76919e686f04d30396e26289287",
  "result": {
    "markdown": "---\ntitle: \"Foundations of Statistical Estimation\"\ndescription: \"We cover the theoretical concepts of statistical estimation.\"\ndate: 2/03/25\nformat: \n  html: \n    toc: true\n    toc_float: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n[cn03 Blank notes file](../_blank_notes/cn03-SRS_blank.qmd)\n\n# Introduction\n\nIn this section we review the basic concepts underlying the selection of an estimator of a population parameter, the method for evaluating its goodness, and the concepts involved in interval estimation. Because the bias and the variance of estimators determine their goodness, we need to review the basic ideas concerned with the expectation and variance of a random variable. This set of notes follows Lohr 2.2\n\n::: {.callout-note appearance=\"minimal\"}\n*Reminder: Most people come into this class with different pre-requisite classes and instructors. This recap helps ensure that we are all on the same page with concepts and notation that may be new for some.*\n\nThese notes require the `StatisticsPhD.csv` data file. [I don't feel like loading packages in this file so I'm using the `::` shortcut method to access the `here` and `read_csv` functions to load in the data.]{.aside}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstat.phd <- readr::read_csv(here::here(\"data\", \"StatisticsPhD.csv\"))\n```\n:::\n\n:::\n\nStatistical inference centers around using information from a sample to understand what might be true about the entire population of interest. If all we see are the data in the sample, what conclusions can we draw about the population? How sure are we about the accuracy of those conclusions?\n\nOn April 29, 2011, Prince William married Kate Middleton in London. The Pew Research Center reports that 34% of US adults watched some or all of the royal wedding. How do we know that 34% of all US adults watched? Did anyone ask **you** if you watched it? In order to know for sure what proportion of US adults watched the wedding, we would need to ask **all** US adults whether or not they watched. This would be very difficult to do. As we will see, however, we can estimate the *population* proportion *parameter* quite accurately with a *sample statistic*, as long as we use a random sample. In the case of the royal wedding, the estimate is based on a poll using a random sample of 1006 US adults.\n\n-   *Statistical inference*: The process of drawing conclusions about the entire population based on the information in the sample.\n-   *Parameter*: A number that describes the entire population. $\\mu$, $p$, $\\tau$\n-   *Statistic*: A number calculated from a sample. $\\bar{x}$, $\\hat{p}$, $\\hat{\\tau}$.\n\nGenerally our goal is to know the value of the population parameter exactly but this usually isn't possible since we usually cannot collect information from the entire population.\n\nInstead we can select a sample from the population, calculate the quantity of interest for the sample, and use this sample statistic to estimate the value for the whole population.\n\n::: callout-warning\n### ‚≠ê You try it\n\nThe US Census states that 27.5% of US adults who are at least 25 years old have a college bachelor's degree or higher. Suppose that in a random sample of $n$=200 US residents who are 25 or older, 58 of them have a college bachelor's degree or higher.\n\nWhat is the population parameter? What is the sample statistic? Use correct notation for your answer.\n:::\n\n::: {.callout-warning icon=\"false\" collapse=\"true\" appearance=\"minimal\"}\n### Solution\n\n**Population Parameter**: $p = .275$\\\n**Sample Statistic**: $\\hat{p} = 58/200 = .29$\n:::\n\nThe value of a statistic for a particular sample gives a *point estimate* of the population parameter. If we only have the one sample and don't know the value of the population parameter, this point estimate is our best estimate of the true value of the population parameter.\n\nAfter we have a little more mathematical terminology and foundation, we'll come back to what we mean by \"best estimate\", and examine how we can determine if something is a \"good\" estimate.\n\n------------------------------------------------------------------------\n\n# Expected Values\n\nGenerally we want to understand some measured characteristic $y$ about the units in the population.\n\n::: {.callout-tip icon=\"true\"}\n### Example: University parking garages\n\nConsider the population of 4 universities $U$ in the North State (Humboldt, Chico, Sac, Davis), and we are interested in knowing the total number of campus owned parking garages at these campuses.\n\n$$ \nU = \\{A, B, C, D\\}\n$$\n:::\n\nThe population total $\\tau$ would then be calculated as\n\n$$\n\\tau = \\sum_{i=1}^{N}y_{i}\n$$ where the $\\sum$ symbols means to sum over all the values of $y_{i}$ for all $i=1, \\ldots, N$ units in the population.\n\nHowever the nature of sampling means that $y_{i}$ is not observed for every unit in the population. In this case, we are going to consider a sample $S$ of 2 campuses. There are six different ways 2 units could be chosen from $U$. $$\nS_{1} = \\{A,B\\}, S_{2} = \\{A,C\\}, S_{3} = \\{A,D\\}, \\\\\nS_{4} = \\{B,C\\}, S_{5} = \\{B,D\\}, S_{6} = \\{C,D\\}\n$$\n\nThen we estimate the population value $\\tau$ using an estimate $\\hat{t}$, where we sum over all the values of $y_{i}$ when that $i^{th}$ campus is present in the chosen sample.\n\n$$\n\\hat{t} = \\sum_{i \\in S}y_{i}\n$$\n\nLet's assume we know the number of parking garages at each campus $y_{A} = 2, y_{B} = 4, y_{C} = 1, y_{D} = 3$. We can build a **sampling distribution**, that is a distribution of all possible values for $\\hat{t}$ based on this design. [A **sampling distribution** is an example of a discrete probability distribution]{.aside}\n\nThe total from sample 1 $\\hat{t}_{1} = y_{A} + y_{B} = 6$ is the total calculated from campus A and B, sample 2 total is $\\hat{t}_{2} = y_{A} + y_{C} = 3$, and so forth. Doing this for all 6 samples results in the following graph:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](cn02-statistical_foundations_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n|  |  |  |  |  |  |\n|------------|:----------:|:----------:|:----------:|:----------:|:----------:|\n| k | 3 | 4 | 5 | 6 | 7 |\n| $P(\\hat{t} = k)$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{3}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ |\n\n: Sampling distribution for the estimated sample total $\\hat{t}$ number of parking garages on two randomly selected college campuses in the north state.\n\nThe only reason we can calculate these probabilities exactly is because we knew the full population and were able to enumerate all possible combinations of samples. In practice we use statistical theory such as the Central Limit theorem to describe the characteristics of sampling distributions so that we can use them to estimate quantities such as means and variances.\n\n::: callout-important\n### Definition: Expected Value\n\n[The **Expected Value** is the mean of the **sampling distribution**]{.aside} The expected value of the statistic is the weighted average of the possible sample values of the statistic, weighted by the probability that particular value of the statistic would occur. $$\nE[y]=\\sum_{y}y*p(y)\n$$\n\nwhere $p(y)$ is the probability of each value of $y$ occurring.\n:::\n\n::: {.callout-tip icon=\"false\"}\n### Example calculation of $E(y)$ from a discrete probability distribution\n\n| y   | p(y) |\n|-----|------|\n| 1   | .1   |\n| 2   | .5   |\n| 3   | .4   |\n\n$E[y]=\\sum_{y}yp(y) = (1)(.1) + (2)(.5) + (3)(.4) = 2.3$\n:::\n\nWe can use R to calculate this for us.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- c(1,2,3)        # Define a vector y of data values\np.y <- c(.1, .5, .4) # define a vector of probabilities\n(E.y <- sum(y*p.y))  # multiply the elements of y and p.y, then sum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.3\n```\n:::\n:::\n\n\n::: callout-important\n### Definition: Population Mean\n\nThe population mean $\\mu$ is defined as the expected value of y.\n\n$$E[y]=\\mu$$\n:::\n\n::: callout-important\n### Definition: Population Variance & Standard Deviation\n\n$$\\sigma^{2} = V(y)=E[y-\\mu]^{2}=\\sum_{y}\\left(y-\\mu\\right)^{2}*p(y)$$\n\nThe variability of measurements in a population can be measured by the **variance** which is defined as the average squared deviation from the mean between a randomly selected measurement $y$ and its mean value $\\mu$.\n\nThe standard deviation is defined as the square root of the variance, and is denoted $\\sigma$.\n\nOften it's more convenient to calculate the variance using this formulation:\n\n$$V(y)=E[y^2] - E[y]^{2}$$\n:::\n\nUsing R on the probability distribution defined above this would look like;\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny.sq <- y^2              # square y\nE.y.sq <- sum(y.sq*p.y)  # calculate E(y^2)\n(Var.y <- E.y.sq - E.y^2)  # Calculate the variance of y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.41\n```\n:::\n:::\n\n\n::: {.callout-warning icon=\"false\"}\n### ‚≠ê You try it\n\nWhen a few data points are repeated in a data set, the results are often arrayed in a frequency table. For example, a quiz given to each of 25 students was graded on a four-point scale (0,1,2,3), 3 being a perfect score.\n\nConsider the following results: 16 students scored a 3, 4 students scored a 2, 2 students scored a 1, and 3 scored a 0. Assume this distribution of scores is representative of the population distribution.\n\n| Score (Y) | Frequency | Proportion |\n|-----------|-----------|------------|\n| 3         | 16        | .64        |\n| 2         | 4         | .16        |\n| 1         | 2         | .08        |\n| 0         | 3         | .12        |\n\nCalculate the population mean and standard deviation\n:::\n\n::: {.callout-warning icon=\"false\" collapse=\"true\" appearance=\"minimal\"}\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- 3:0\np <- c(.64, .16, .08, .12)\n\n(y.bar.p <- sum(y*p))  # mean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.32\n```\n:::\n\n```{.r .cell-code}\nsqrt(sum(p*(y-y.bar.p)^2)) #sd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.047664\n```\n:::\n:::\n\n:::\n\n# The Finite Population Case\n\nThe previous section develops results for random sampling from a population considered to be infinite. In such situations, each sampled element has the same chance of being selected and the selections are independent of one another.\n\n-   Most sampling problems don't live in an infinite world, but the population is usually finite although it may be quite large.\n-   In addition, we may want to take into consideration varying the probabilities with which the units are sampled.\n\nIn *probability sampling*, each possible sample $S$ has a known probability $\\delta$ of being chosen. Let's return to the university parking garage example and use $\\delta_{1} = P(\\S_{1}) = 1/3, \\delta_{2} = P(S_{2}) = 1/6, \\delta_{6} = P(S_{6}) = 1/2$ and the rest have probability 0.\n\nAdditionally, each unit $i$ within the universe $U$ has a $\\pi_{i} \\gt 0$ probability of appearing in the selected sample. These unit probabilities are a function of the probability of selecting that random sample.\n\n|             |                                        |                     |\n|:-----------:|:--------------------------------------:|:-------------------:|\n| $\\pi_{A}$ = | $\\delta_{1} + \\delta_{2} + \\delta_{3}$ | 1/3 + 1/6 + 0 = 1/2 |\n| $\\pi_{B}$ = | $\\delta_{1} + \\delta_{4} + \\delta_{5}$ |  1/3 + 0 + 0 = 1/3  |\n| $\\pi_{C}$ = | $\\delta_{2} + \\delta_{4} + \\delta_{6}$ | 1/6 + 0 + 1/2 = 2/3 |\n| $\\pi_{D}$ = | $\\delta_{3} + \\delta_{5} + \\delta_{6}$ |  0 + 0 + 1/2 = 1/2  |\n\nSo in this case, not all units (campuses) $i$ have the sa me probability of appearing in our sample.\n\n::: {.callout-warning icon=\"false\"}\n### ‚≠ê You try it\n\nChange the values of $\\delta$ for each of the 6 possible samples of $U$ above, so that each sample has a non-zero probabilty of being chosen. Then recalculate the observational probabilities $\\pi_{A}-\\pi_{D}$.\n:::\n\nProbability sampling:\n\n-   is more difficult than some sampling methods such as convenience sample\n-   is less prone to selection biases discussed earlier\n-   guarantees each unit in the population has a chance of appearing in the sample\n-   allows us to calculate the precision of statistics calculated from the sample\n\n::: {.callout-tip icon=\"true\"}\n### Example: Estimate total number of job openings.\n\nSuppose, for example, we want to estimate the total number of job openings in a city by sampling industrial companies from within that city.\n\n| Company | Jobs ($y$) | Size of company |\n|---------|------------|-----------------|\n| A       | 3          | 70              |\n| B       | 10         | 90              |\n| C       | 25         | 120             |\n| D       | 61         | 300             |\n\nThe population total of job openings is $\\tau= 99$. But of course we don't know this, so we want to take a sample of $n=2$ firms and count the number of jobs at each firm $y$ as a way to estimate this total.\n:::\n\n[Note: The mathematical definition of unbiased is not the same thing as selection biased discussed earlier. The math definition of bias will be discussed below.]{.aside} An **unbiased estimator** of the population total, $\\tau$, is given by\n\n$$\n\\hat{t}=\\frac{1}{n}\\sum^{n}_{i=1}\\frac{y_{i}}{\\pi_{i}}\n$$\n\nIf all companies have equal probability of being selected, then the subset of companies $S = \\{A, B\\}$ have the same chance of being selected as any other subset of two companies such as $S = \\{A, C\\}$. This can occur when we sample one company out of the $N$ companies in the population randomly with replacement, which means $\\pi_{i} = 1/N$. This allows us to rewrite this equation as\n\n$$\n\\hat{t} = \\frac{1}{n}\\sum^{n}_{i=1}\\frac{y_{i}}{(1/N)} = \\frac{N}{n} \\sum^{n}_{i=1}y_{i} = N\\bar{y}\n$$\n\nThus the estimate of the total job openings based on the $S = \\{A, B\\}$ sample would be:\n\n$$\n\\hat{t} = N\\bar{y} = 4*(13/2) = 26\n$$\n\nNote that the size of these companies vary quite a lot, and the number of job openings will likely be directly dependent on the size of the company Thus, we might improve the sample if large companies are more likely to be included in the sample. One practical way to choose the $\\pi_{i}$, is to choose them proportional to a known measurement that is highly correlated with $y_{i}$. In this case, that would be the company size.\n\n::: {.callout-tip icon=\"true\"}\n| Company | Jobs ($y$) | Size of company | $\\pi_{i}$ |\n|---------|------------|-----------------|-----------|\n| A       | 3          | 70              | 70/580    |\n| B       | 10         | 90              | 90/580    |\n| C       | 25         | 120             | 120/580   |\n| D       | 61         | 300             | 300/580   |\n:::\n\nIf we adjust $y_{i}$ based on the probability of selecting firms A and B,\n\n$$\n\\hat{t}=\\frac{1}{n}\\sum^{n}_{i=1}\\frac{y_{i}}{\\pi_{i}} = \\frac{1}{2}\\Big[\\frac{3}{70/580} + \\frac{10}{90/580}\\Big]  = 44.7\n$$\n\n‚ùì Was this a good estimate of $\\tau$? Why or why not?\n\n> No! The population total $\\tau$ is nearly double that of the estimate. However, the weighted mean is much closer to the true value than the unweighted mean is.\n\n# Variability of Estimates\n\n::: {.callout-note appearance=\"minimal\"}\n### ‚ùìüë•Turn and talk\n\nWhy do we usually think of a parameter $\\mu$ or $p$ as a fixed value, while the sample statistic $\\bar{x}$ or $\\hat{p}$ as a random variable?\n:::\n\n> write your answer in HackMD.\n\nAlong with the point estimate we also want to know how accurate we can expect the point estimate to be. In other words, if we took another random sample of the same size from the population, is the point estimate from this new sample likely to be similar to the first point estimate or are they likely to be far apart.\n\n::: {.callout-tip icon=\"true\"}\n### Example: Enrollment in Graduate Programs in Statistics\n\nGraduate programs in statistics sometimes pay their graduate students, which means that many graduate students in statistics are able to attend graduate school tuition free with an assistantship or fellowship. There are 82 US statistics doctoral programs for which enrollment data were available. The data set `StatisticsPhD` lists all these schools together with the total enrollment of full-time graduate students in each program in 2009.\n\n‚ùì What is the average full-time graduate student enrollment in US statistics doctoral programs in 2009?\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(stat.phd) #always look at your imported data to check for import errors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 3\n  University                      Department    FTGradEnrollment\n  <chr>                           <chr>                    <dbl>\n1 Baylor University               Statistics                  26\n2 Boston University               Biostatistics               39\n3 Brown University                Biostatistics               21\n4 Carnegie Mellon University      Statistics                  39\n5 Case Western Reserve University Statistics                  11\n6 Colorado State University       Statistics                  14\n```\n:::\n\n```{.r .cell-code}\nmean(stat.phd$FTGradEnrollment)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 53.53659\n```\n:::\n:::\n\n\nBased on the data set, the mean enrollment in 2009 is 53.54 full-time graduate students. Because this is the mean for the entire population of all US statistics doctoral programs for which data were available that year, we have that $\\mu=53.54$ students.\n\n‚≠ê Use the code below take a random sample of 10 programs from the data file then calculate the mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy.sample.programs <- sample(stat.phd$FTGradEnrollment, size=10)\nmean(my.sample.programs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 44.9\n```\n:::\n:::\n\n\n::: {.callout-note appearance=\"minimal\"}\n### ‚ùìüë•Turn to your neighbor and discuss the following questions\n\n-   Did everyone get the same sample mean?\n-   Does your sample mean exactly equal the population mean?\n-   If you took another sample of 10, would you get the same sample mean? Why?\n-   If we created a histogram of all our sample means, what would it look like? Where would it be centered at? What is the spread of the histogram?\n\nSummarize your findings in HackMD and compare your answer to other groups.\n:::\n\nKnowing the behavior of of repeated sample statistics (like the mean in the prior example) is critically important. Let's dig into this a little more by repeating this sampling experiment many times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany.means <- replicate(n=100, {\n  my.sample.programs <- sample(stat.phd$FTGradEnrollment, size=10)\n  mean(my.sample.programs)\n})\n```\n:::\n\n\nLet's visualize the distribution of all those sample means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(many.means)\n```\n\n::: {.cell-output-display}\n![](cn02-statistical_foundations_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(many.means)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  33.70   45.23   51.60   53.79   61.98   86.20 \n```\n:::\n:::\n\n\nCharacteristics of this distribution:\n\n-   *Shape*: The distribution of average enrollment isn't quite normal, there seems to perhaps be two peaks?\n-   *Center*: The average enrollment is 53.79\n-   *Spread*: Average enrollment ranges from 33.7, 86.2.\n\nüéâ We have just created a *sampling distribution* of the sample mean.\n\n[The proof of the CLT is generally seen in a first graduate Statistics class that uses convergence in distribution concepts from MATH 420.]{.aside} In elementary statistics you were introduced to the \"Central Limit Theorem\" (CLT). The CLT states that the sampling distribution of $\\bar{y}$ should be centered at $\\mu$ ($E(\\bar{y}) = \\mu)$) with a variance of $\\sigma^{2}/n$. It also states that the shape of the distribution should be approximately Normal for a large $n$.\n\n# Properties of Estimators\n\nIn general, suppose that $\\hat{\\theta}$ is an estimator of the parameter $\\theta$. Two properties that we would like $\\hat{\\theta}$ to have are [The symbol $\\theta$ is used as a generic parameter, like how $x$ tends to be used for a generic unknown value or variable.]{.aside}\n\n1.  The estimate is **unbiased**: $E(\\hat{\\theta}) = \\theta$.\n2.  The estimate is **precise**: $Var(\\hat{\\theta})$ is small.\n\nTo clarify:\n\n-   **Measurement bias** means that the $y$'s are measured inaccurately.\n    -   That means an estimator of a total $\\tau$ calculated as $\\hat{t} = \\sum_{i \\in U} y_{i}$ where $U$ is the entire universe, then $\\hat{t}$ itself would not be the true total of interest.\n    -   Trying to estimate heights of students, but your ruler is always off 3 cm\n-   **Estimation bias** means that the estimator chosen resulted in a bias.\n    -   If we calculated the total as $t' = \\sum_{i \\in L} y_{i}$ from a random sample of $L$ units in the universe, $\\hat{t'}$ would be biased.\n    -   Trying to estimate heights of students by taking a sample of the shorter students only.\n\nIf two unbiased estimators are available for $\\theta$ we generally prefer the one with the smaller variance.\n\nBecause sometimes we use biased estimators, we often use the **Mean Squared Error (MSE)** instead of the variance to estimate the **accuracy**.\n\n::: callout-important\n### Definition: Bias, Variance, Accuracy\n\n$$\\mbox{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$$\n\n$$V(\\hat{\\theta})  = E \\Big[(\\hat{\\theta} - E[\\hat{\\theta}])^2\\Big]$$\n\n$$\\mbox{MSE}(\\hat{\\theta}) = V(\\hat{\\theta}) + [Bias(\\hat{\\theta})]^2$$\n:::\n\n![Textbook Figure 2.3. Unbiased, precise and accurate archers.](../figs/fig2_3.jpg)\n\n-   A is **unbiased**. The average position of all arrows is at the center of the target.\n-   B is **precise** but not unbiased. All arrows are close together but systematically away from the center.\n-   C is **accurate**. All arrows are close together and near the center of the target.\n\n# Interval Estimation\n\nIn general, it is usually not enough to just give a point estimate when estimating a population parameter. Why?\n\n## Standard Error\n\n::: callout-important\n### Definition: Standard Error\n\nThe *standard error* of a statistic is the standard deviation of the sample statistic. In other words, the standard deviation of the sampling distribution.\n:::\n\nThe standard error of a statistic tells us how much the sample statistic will vary from sample to sample. In situations like above where we can examine the distribution of the sample statistic using simulation, we can estimate the standard error by taking the sample standard deviation of the sampling distribution. In other situations we can use closed form mathematical formulas to calculate the standard error.\n\n::: {.callout-tip icon=\"true\"}\n### Example Grad program example cont.\n\nEstimate the standard error for the mean enrollment in statistics PhD programs for a sample size of 10 and also a sample size of 20.\n:::\n\n[I used the base pipe `|>` here to pass the results of the replicate function into the `sd()` function]{.aside}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(many.means) #because the example above already had n=10\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11.41401\n```\n:::\n\n```{.r .cell-code}\nreplicate(n=100, {\n  my.sample.programs <- sample(stat.phd$FTGradEnrollment, size=20)\n  mean(my.sample.programs)\n}) |> sd()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.482007\n```\n:::\n:::\n\n\n## Confidence Intervals\n\nWhen the distributions are relatively symmetric and bell-shaped, the 95% rule tells us that approximately 95% of the data values fall within two standard deviations of the mean. Applying the 95% rule to sampling distributions, we see that about 95% of the sample statistics will fall within two standard errors of the mean. We use this rule many times to form what we call an approximate 95% confidence interval which gives us a range for which which we are quite confident that captures the true parameter we are trying to estimate.\n\nWhen using a formula to calculate an approximate 95% confidence interval, use $2*SE$ as the margin of error.\n\n::: {.callout-tip icon=\"true\"}\n### CI for PhD program enrollment\n\nBased on our example, what would be a 95% confidence interval for $\\mu$ the true mean total enrollment for PhD programs in statistics. Interpret this confidence interval in context of the problem.\n:::\n\n[Wrapping the entire line of code in `()` will execute that code AND print out the results. This lets me see the results in the rendered document AND store the results in an object to call later, like in my sentence response below.]{.aside}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(LCL <- mean(many.means) - 2*sd(many.means))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30.95899\n```\n:::\n\n```{.r .cell-code}\n(UCL <- mean(many.means) + 2*sd(many.means))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 76.61501\n```\n:::\n:::\n\n\n[Every time I compile these notes, I draw a different sample and will get a different numbers. To avoid conflicts in my written response, and what the code shows, I use *inline R code* here. See the RStudio *Help --\\> Markdown Quick Reference* for more information.]{.aside}\n\n> We can be 95% confident that the true mean total enrollment for PhD programs in statistics is covered by the interval (31 , 76.6).\n\n::: {.callout-note appearance=\"minimal\"}\n### ‚ùìüë• Turn & talk\n\nConsider the following interpretation of the above confidence interval.\n\n> We can be 95% confident that PhD programs in Statistics have a total enrollment of between 31 and 76.6 students.\n\nWhy is this an incorrect interpretation?\n:::\n",
    "supporting": [
      "cn02-statistical_foundations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}